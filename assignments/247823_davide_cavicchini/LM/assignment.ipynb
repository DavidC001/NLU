{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"MclkJGBkKUff"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import numpy as np\n","import torch.utils.data as data\n","from functools import partial\n","from torch.utils.data import DataLoader\n","from tqdm import tqdm\n","import copy\n","from torch.utils.tensorboard import SummaryWriter\n","import torch\n","import torch.nn as nn\n","from typing import *\n","from torch.optim import Optimizer"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23695,"status":"ok","timestamp":1712590892666,"user":{"displayName":"Davide Cavicchini","userId":"03391175637593717529"},"user_tz":-120},"id":"Sc1podZSKUfh","outputId":"09c333b2-66e7-4e2c-b887-1571a27f29a4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["#from google.colab import drive\n","#drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-eF803flKUfi"},"outputs":[],"source":["tensorboard_folder = 'tensorboard'\n","models_folder = 'models'\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TbAocSl_KUfi"},"outputs":[],"source":["batch_size = 256"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YNR6UcBflKfJ"},"outputs":[],"source":["run_exp = [0,0,0,0,0,0,1]"]},{"cell_type":"markdown","metadata":{"id":"GLw0Oc22KUfj"},"source":["# Data loading"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QHyGt8SHKUfk"},"outputs":[],"source":["# Loading the corpus\n","\n","def read_file(path, eos_token=\"<eos>\"):\n","    output = []\n","    with open(path, \"r\") as f:\n","        for line in f.readlines():\n","            output.append(line.strip() + \" \" + eos_token)\n","    return output\n","\n","# Vocab with tokens to ids\n","def get_vocab(corpus, special_tokens=[]):\n","    output = {}\n","    i = 0\n","    for st in special_tokens:\n","        output[st] = i\n","        i += 1\n","    for sentence in corpus:\n","        for w in sentence.split():\n","            if w not in output:\n","                output[w] = i\n","                i += 1\n","    return output"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1179,"status":"ok","timestamp":1712590893843,"user":{"displayName":"Davide Cavicchini","userId":"03391175637593717529"},"user_tz":-120},"id":"X4SIkFUxKUfk","outputId":"3c76115e-6190-4667-ee0f-915b32b8eb15"},"outputs":[{"name":"stdout","output_type":"stream","text":["--2024-04-08 15:41:31--  https://raw.githubusercontent.com/BrownFortress/NLU-2024-Labs/main/labs/dataset/PennTreeBank/ptb.test.txt\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 449945 (439K) [text/plain]\n","Saving to: ‘dataset/PennTreeBank/ptb.test.txt’\n","\n","\rptb.test.txt          0%[                    ]       0  --.-KB/s               \rptb.test.txt        100%[===================>] 439.40K  --.-KB/s    in 0.02s   \n","\n","2024-04-08 15:41:32 (18.2 MB/s) - ‘dataset/PennTreeBank/ptb.test.txt’ saved [449945/449945]\n","\n","--2024-04-08 15:41:32--  https://raw.githubusercontent.com/BrownFortress/NLU-2024-Labs/main/labs/dataset/PennTreeBank/ptb.valid.txt\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 399782 (390K) [text/plain]\n","Saving to: ‘dataset/PennTreeBank/ptb.valid.txt’\n","\n","ptb.valid.txt       100%[===================>] 390.41K  --.-KB/s    in 0.02s   \n","\n","2024-04-08 15:41:32 (16.2 MB/s) - ‘dataset/PennTreeBank/ptb.valid.txt’ saved [399782/399782]\n","\n","--2024-04-08 15:41:32--  https://raw.githubusercontent.com/BrownFortress/NLU-2024-Labs/main/labs/dataset/PennTreeBank/ptb.train.txt\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 5101618 (4.9M) [text/plain]\n","Saving to: ‘dataset/PennTreeBank/ptb.train.txt’\n","\n","ptb.train.txt       100%[===================>]   4.87M  --.-KB/s    in 0.04s   \n","\n","2024-04-08 15:41:32 (116 MB/s) - ‘dataset/PennTreeBank/ptb.train.txt’ saved [5101618/5101618]\n","\n"]}],"source":["# If you are using Colab, run these lines\n","#!wget -P dataset/PennTreeBank https://raw.githubusercontent.com/BrownFortress/NLU-2024-Labs/main/labs/dataset/PennTreeBank/ptb.test.txt\n","#!wget -P dataset/PennTreeBank https://raw.githubusercontent.com/BrownFortress/NLU-2024-Labs/main/labs/dataset/PennTreeBank/ptb.valid.txt\n","#!wget -P dataset/PennTreeBank https://raw.githubusercontent.com/BrownFortress/NLU-2024-Labs/main/labs/dataset/PennTreeBank/ptb.train.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QpVE3Gd8KUfl"},"outputs":[],"source":["\n","train_raw = read_file(\"dataset/PennTreeBank/ptb.train.txt\")\n","dev_raw = read_file(\"dataset/PennTreeBank/ptb.valid.txt\")\n","test_raw = read_file(\"dataset/PennTreeBank/ptb.test.txt\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bouDf_aMKUfl"},"outputs":[],"source":["# Vocab is computed only on training set\n","# We add two special tokens end of sentence and padding\n","vocab = get_vocab(train_raw, [\"<pad>\", \"<eos>\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"arxngdWyKUfl"},"outputs":[],"source":["# This class computes and stores our vocab\n","# Word to ids and ids to word\n","class Lang():\n","    def __init__(self, corpus, special_tokens=[]):\n","        self.word2id = self.get_vocab(corpus, special_tokens)\n","        self.id2word = {v:k for k, v in self.word2id.items()}\n","    def get_vocab(self, corpus, special_tokens=[]):\n","        output = {}\n","        i = 0\n","        for st in special_tokens:\n","            output[st] = i\n","            i += 1\n","        for sentence in corpus:\n","            for w in sentence.split():\n","                if w not in output:\n","                    output[w] = i\n","                    i += 1\n","        return output\n","\n","lang = Lang(train_raw, [\"<pad>\", \"<eos>\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"843xfHh-KUfm"},"outputs":[],"source":["class PennTreeBank (data.Dataset):\n","    # Mandatory methods are __init__, __len__ and __getitem__\n","    def __init__(self, corpus, lang):\n","        self.source = []\n","        self.target = []\n","\n","        for sentence in corpus:\n","            self.source.append(sentence.split()[0:-1]) # We get from the first token till the second-last token\n","            self.target.append(sentence.split()[1:]) # We get from the second token till the last token\n","            # See example in section 6.2\n","\n","        self.source_ids = self.mapping_seq(self.source, lang)\n","        self.target_ids = self.mapping_seq(self.target, lang)\n","\n","    def __len__(self):\n","        return len(self.source)\n","\n","    def __getitem__(self, idx):\n","        src= torch.LongTensor(self.source_ids[idx])\n","        trg = torch.LongTensor(self.target_ids[idx])\n","        sample = {'source': src, 'target': trg}\n","        return sample\n","\n","    # Auxiliary methods\n","\n","    def mapping_seq(self, data, lang): # Map sequences of tokens to corresponding computed in Lang class\n","        res = []\n","        for seq in data:\n","            tmp_seq = []\n","            for x in seq:\n","                if x in lang.word2id:\n","                    tmp_seq.append(lang.word2id[x])\n","                else:\n","                    print('OOV found!')\n","                    print('You have to deal with that') # PennTreeBank doesn't have OOV but \"Trust is good, control is better!\"\n","                    break\n","            res.append(tmp_seq)\n","        return res"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hYPUW3N9KUfm"},"outputs":[],"source":["train_dataset = PennTreeBank(train_raw, lang)\n","dev_dataset = PennTreeBank(dev_raw, lang)\n","test_dataset = PennTreeBank(test_raw, lang)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ypuc4zOiKUfn"},"outputs":[],"source":["def collate_fn(data, pad_token):\n","    def merge(sequences):\n","        '''\n","        merge from batch * sent_len to batch * max_len\n","        '''\n","        lengths = [len(seq) for seq in sequences]\n","        max_len = 1 if max(lengths)==0 else max(lengths)\n","        # Pad token is zero in our case\n","        # So we create a matrix full of PAD_TOKEN (i.e. 0) with the shape\n","        # batch_size X maximum length of a sequence\n","        padded_seqs = torch.LongTensor(len(sequences),max_len).fill_(pad_token)\n","        for i, seq in enumerate(sequences):\n","            end = lengths[i]\n","            padded_seqs[i, :end] = seq # We copy each sequence into the matrix\n","        padded_seqs = padded_seqs.detach()  # We remove these tensors from the computational graph\n","        return padded_seqs, lengths\n","\n","    # Sort data by seq lengths\n","\n","    data.sort(key=lambda x: len(x[\"source\"]), reverse=True)\n","    new_item = {}\n","    for key in data[0].keys():\n","        new_item[key] = [d[key] for d in data]\n","\n","    source, _ = merge(new_item[\"source\"])\n","    target, lengths = merge(new_item[\"target\"])\n","\n","    new_item[\"source\"] = source.to(device)\n","    new_item[\"target\"] = target.to(device)\n","    new_item[\"number_tokens\"] = sum(lengths)\n","    return new_item\n"]},{"cell_type":"markdown","metadata":{"id":"ToGSWEprKUfn"},"source":["# Train Loop"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ish22hBbKUfn"},"outputs":[],"source":["criterion_train = nn.CrossEntropyLoss(ignore_index=lang.word2id[\"<pad>\"])\n","criterion_eval = nn.CrossEntropyLoss(ignore_index=lang.word2id[\"<pad>\"], reduction='sum')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_kBN03dpKUfo"},"outputs":[],"source":["import math\n","def train_loop(data, optimizer, criterion, model, clip=5):\n","    model.train()\n","    loss_array = []\n","    number_of_tokens = []\n","\n","    for sample in data:\n","        optimizer.zero_grad() # Zeroing the gradient\n","        output = model(sample['source'])\n","        loss = criterion(output, sample['target'])\n","        loss_array.append(loss.item() * sample[\"number_tokens\"])\n","        number_of_tokens.append(sample[\"number_tokens\"])\n","        loss.backward() # Compute the gradient, deleting the computational graph\n","        # clip the gradient to avoid explosioning gradients\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n","        optimizer.step() # Update the weights\n","\n","    return sum(loss_array)/sum(number_of_tokens)\n","\n","def eval_loop(data, eval_criterion, model):\n","    model.eval()\n","    loss_to_return = []\n","    loss_array = []\n","    number_of_tokens = []\n","    # softmax = nn.Softmax(dim=1) # Use Softmax if you need the actual probability\n","    with torch.no_grad(): # It used to avoid the creation of computational graph\n","        for sample in data:\n","            output = model(sample['source'])\n","            loss = eval_criterion(output, sample['target'])\n","            loss_array.append(loss.item())\n","            number_of_tokens.append(sample[\"number_tokens\"])\n","\n","    ppl = math.exp(sum(loss_array) / sum(number_of_tokens))\n","    loss_to_return = sum(loss_array) / sum(number_of_tokens)\n","    return ppl, loss_to_return\n","\n","def init_weights(mat):\n","    for m in mat.modules():\n","        if type(m) in [nn.GRU, nn.LSTM, nn.RNN]:\n","            for name, param in m.named_parameters():\n","                if 'weight_ih' in name:\n","                    for idx in range(4):\n","                        mul = param.shape[0]//4\n","                        torch.nn.init.xavier_uniform_(param[idx*mul:(idx+1)*mul])\n","                elif 'weight_hh' in name:\n","                    for idx in range(4):\n","                        mul = param.shape[0]//4\n","                        torch.nn.init.orthogonal_(param[idx*mul:(idx+1)*mul])\n","                elif 'bias' in name:\n","                    param.data.fill_(0)\n","        else:\n","            if type(m) in [nn.Linear]:\n","                torch.nn.init.uniform_(m.weight, -0.01, 0.01)\n","                if m.bias != None:\n","                    m.bias.data.fill_(0.01)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AJtF0hKlKUfo"},"outputs":[],"source":["def train(model, optimizer, exp_name, clip=5, epochs=100, patience=3, batch_size=64):\n","    pat = patience\n","\n","    train_loader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=partial(collate_fn, pad_token=lang.word2id[\"<pad>\"]),  shuffle=True)\n","    dev_loader = DataLoader(dev_dataset, batch_size=batch_size, collate_fn=partial(collate_fn, pad_token=lang.word2id[\"<pad>\"]))\n","    test_loader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=partial(collate_fn, pad_token=lang.word2id[\"<pad>\"]))\n","\n","    writer = SummaryWriter(tensorboard_folder+'/'+exp_name)\n","\n","    best_ppl = math.inf\n","\n","    pbar = tqdm(range(1,epochs))\n","    #If the PPL is too high try to change the learning rate\n","    for epoch in pbar:\n","        loss = train_loop(train_loader, optimizer, criterion_train, model, clip)\n","\n","        if epoch % 1 == 0:\n","            ppl_dev, loss_dev = eval_loop(dev_loader, criterion_eval, model)\n","            pbar.set_description(\"PPL: %f\" % ppl_dev)\n","\n","            writer.add_scalar('Loss/train', np.asarray(loss).mean(), epoch)\n","            writer.add_scalar('Loss/dev', np.asarray(loss_dev).mean(), epoch)\n","            writer.add_scalar('PPL/dev', ppl_dev, epoch)\n","\n","            if  ppl_dev < best_ppl: # the lower, the better\n","                best_ppl = ppl_dev\n","                best_model = copy.deepcopy(model)\n","                pat = patience\n","            else:\n","                pat -= 1\n","\n","            if pat <= 0: # Early stopping with patience\n","                break # Not nice but it keeps the code clean\n","\n","    best_model.to(device)\n","    final_ppl,  _ = eval_loop(test_loader, criterion_eval, best_model)\n","    print('Test ppl: ', final_ppl)\n","\n","    writer.add_scalar('PPL/test', final_ppl, 0)\n","\n","    # Save the best model\n","    torch.save(best_model.state_dict(), models_folder+'/'+exp_name+'.pt')\n","\n","    writer.close()"]},{"cell_type":"markdown","metadata":{"id":"864ZIfxlKUfo"},"source":["# Baseline"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZfeXfUW8KUfp"},"outputs":[],"source":["name = \"baseline_higherLR\""]},{"cell_type":"markdown","metadata":{"id":"RB48WdOyKUfp"},"source":["## Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yi4z22vJKUfp"},"outputs":[],"source":["class LM_RNN(nn.Module):\n","    def __init__(self, emb_size, hidden_size, output_size, pad_index=0, out_dropout=0.1,\n","                 emb_dropout=0.1, n_layers=1):\n","        super(LM_RNN, self).__init__()\n","        # Token ids to vectors, we will better see this in the next lab\n","        self.embedding = nn.Embedding(output_size, emb_size, padding_idx=pad_index)\n","        # Pytorch's RNN layer: https://pytorch.org/docs/stable/generated/torch.nn.RNN.html\n","        self.rnn = nn.RNN(emb_size, hidden_size, n_layers, bidirectional=False, batch_first=True)\n","        self.pad_token = pad_index\n","        # Linear layer to project the hidden layer to our output space\n","        self.output = nn.Linear(hidden_size, output_size)\n","\n","    def forward(self, input_sequence):\n","        emb = self.embedding(input_sequence)\n","        rnn_out, _  = self.rnn(emb)\n","        output = self.output(rnn_out).permute(0,2,1)\n","        return output"]},{"cell_type":"markdown","metadata":{"id":"9Sx7yRgaKUfp"},"source":["## Params"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9qjN7YfnKUfp"},"outputs":[],"source":["hid_size = 300\n","emb_size = 300\n","\n","lr = 2\n","clip = 5\n","patience = 3\n","n_epochs = 100\n","\n","vocab_len = len(lang.word2id)\n","\n","model = LM_RNN(emb_size, hid_size, vocab_len, pad_index=lang.word2id[\"<pad>\"]).to(device)\n","model.apply(init_weights)\n","\n","optimizer = optim.SGD(model.parameters(), lr=lr)\n","criterion_train = nn.CrossEntropyLoss(ignore_index=lang.word2id[\"<pad>\"])\n","criterion_eval = nn.CrossEntropyLoss(ignore_index=lang.word2id[\"<pad>\"], reduction='sum')"]},{"cell_type":"markdown","metadata":{"id":"B69kDtnyKUfq"},"source":["## Train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rbcF9EbuKUfq"},"outputs":[],"source":["if (run_exp[0]):\n","  train(model=model, optimizer=optimizer, exp_name=name, clip=clip, epochs=n_epochs, patience=patience, batch_size=batch_size)\n","\n","  #save all parameters (lr, emb_size, hid_size, optimizer, clip, patience, n_epochs)\n","  with open(tensorboard_folder+'/'+name+'/params.txt', 'w') as f:\n","      f.write('lr: {}\\n'.format(optimizer.param_groups[0]['lr']))\n","      f.write('weight_decay: {}\\n'.format(optimizer.param_groups[0]['weight_decay']))\n","      f.write('optimizer: {}\\n'.format(optimizer))\n","      f.write('emb_size: {}\\n'.format(emb_size))\n","      f.write('hid_size: {}\\n'.format(hid_size))\n","      f.write('optimizer: {}\\n'.format(optimizer))\n","      f.write('clip: {}\\n'.format(clip))\n","      f.write('patience: {}\\n'.format(patience))\n","      f.write('n_epochs: {}\\n'.format(n_epochs))\n","      f.write('batch_size: {}\\n'.format(batch_size))"]},{"cell_type":"markdown","metadata":{"id":"3nQAvSUGKUfq"},"source":["# Part 1"]},{"cell_type":"markdown","metadata":{"id":"Z6cPl5m2KUfq"},"source":["In this, you have to modify the baseline LM_RNN by adding a set of techniques that might improve the performance. In this, you have to add one modification at a time incrementally. If adding a modification decreases the performance, you can remove it and move forward with the others. However, in the report, you have to provide and comment on this unsuccessful experiment.  For each of your experiments, you have to print the performance expressed with Perplexity (PPL).\n","<br>\n","One of the important tasks of training a neural network is  hyperparameter optimization. Thus, you have to play with the hyperparameters to minimise the PPL and thus print the results achieved with the best configuration (in particular <b>the learning rate</b>).\n","These are two links to the state-of-the-art papers which use vanilla RNN [paper1](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5947611), [paper2](https://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf).\n","\n","**Mandatory requirements**: For the following experiments the perplexity must be below 250 (***PPL < 250***).\n","\n","1. Replace RNN with a Long-Short Term Memory (LSTM) network --> [link](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html)\n","2. Add two dropout layers: --> [link](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html)\n","    - one after the embedding layer,\n","    - one before the last linear layer\n","3. Replace SGD with AdamW --> [link](https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html)"]},{"cell_type":"markdown","metadata":{"id":"njUBeO_LKUfq"},"source":["## LSTM base"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wI9l7ellKUfq"},"outputs":[],"source":["name = \"LSTM_base_higherLR\""]},{"cell_type":"markdown","metadata":{"id":"fs2xsSZuKUfr"},"source":["### Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E53n07tFKUfr"},"outputs":[],"source":["class LM_LSTM(nn.Module):\n","    def __init__(self, emb_size, hidden_size, output_size, pad_index=0, out_dropout=0.1,\n","                 emb_dropout=0.1, n_layers=1):\n","        super(LM_LSTM, self).__init__()\n","        # Token ids to vectors, we will better see this in the next lab\n","        self.embedding = nn.Embedding(output_size, emb_size, padding_idx=pad_index)\n","        # Pytorch's RNN layer: https://pytorch.org/docs/stable/generated/torch.nn.RNN.html\n","        self.rnn = nn.LSTM(emb_size, hidden_size, n_layers, bidirectional=False, batch_first=True)\n","        self.pad_token = pad_index\n","        # Linear layer to project the hidden layer to our output space\n","        self.output = nn.Linear(hidden_size, output_size)\n","\n","    def forward(self, input_sequence):\n","        emb = self.embedding(input_sequence)\n","        rnn_out, _  = self.rnn(emb)\n","        output = self.output(rnn_out).permute(0,2,1)\n","        return output"]},{"cell_type":"markdown","metadata":{"id":"LbCSEbk2KUfr"},"source":["### Params"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nCyNn6sOKUfr"},"outputs":[],"source":["hid_size = 300\n","emb_size = 300\n","\n","lr = 2\n","clip = 5\n","patience = 3\n","n_epochs = 100\n","\n","vocab_len = len(lang.word2id)\n","\n","model = LM_RNN(emb_size, hid_size, vocab_len, pad_index=lang.word2id[\"<pad>\"]).to(device)\n","model.apply(init_weights)\n","\n","optimizer = optim.SGD(model.parameters(), lr=lr)\n","criterion_train = nn.CrossEntropyLoss(ignore_index=lang.word2id[\"<pad>\"])\n","criterion_eval = nn.CrossEntropyLoss(ignore_index=lang.word2id[\"<pad>\"], reduction='sum')\n"]},{"cell_type":"markdown","metadata":{"id":"Lj_lAUSmKUfr"},"source":["### Train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ong5BzYsKUfs"},"outputs":[],"source":["if (run_exp[1]):\n","  train(model=model, optimizer=optimizer, exp_name=name, clip=clip, epochs=n_epochs, patience=patience, batch_size=batch_size)\n","\n","  #save all parameters (lr, emb_size, hid_size, optimizer, clip, patience, n_epochs)\n","  with open(tensorboard_folder+'/'+name+'/params.txt', 'w') as f:\n","      f.write('lr: {}\\n'.format(optimizer.param_groups[0]['lr']))\n","      f.write('weight_decay: {}\\n'.format(optimizer.param_groups[0]['weight_decay']))\n","      f.write('optimizer: {}\\n'.format(optimizer))\n","      f.write('emb_size: {}\\n'.format(emb_size))\n","      f.write('hid_size: {}\\n'.format(hid_size))\n","      f.write('optimizer: {}\\n'.format(optimizer))\n","      f.write('clip: {}\\n'.format(clip))\n","      f.write('patience: {}\\n'.format(patience))\n","      f.write('n_epochs: {}\\n'.format(n_epochs))\n","      f.write('batch_size: {}\\n'.format(batch_size))"]},{"cell_type":"markdown","metadata":{"id":"nH-j_QD8KUfs"},"source":["## LSTM dropout"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KvILdFP_KUfs"},"outputs":[],"source":["name = \"LSTM_dropout_higherLR\""]},{"cell_type":"markdown","metadata":{"id":"GSf-0VbDKUfs"},"source":["### Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sMwCJY7TKUfs"},"outputs":[],"source":["class LM_LSTM_drop(nn.Module):\n","    def __init__(self, emb_size, hidden_size, output_size, pad_index=0, out_dropout=0.1,\n","                 emb_dropout=0.1, n_layers=1):\n","        super(LM_LSTM_drop, self).__init__()\n","        # Token ids to vectors, we will better see this in the next lab\n","        self.embedding = nn.Embedding(output_size, emb_size, padding_idx=pad_index)\n","        # Pytorch's RNN layer: https://pytorch.org/docs/stable/generated/torch.nn.RNN.html\n","        self.rnn = nn.LSTM(emb_size, hidden_size, n_layers, bidirectional=False, batch_first=True)\n","        self.pad_token = pad_index\n","        self.emb_dropout = nn.Dropout(emb_dropout)\n","        self.out_dropout = nn.Dropout(out_dropout)\n","        # Linear layer to project the hidden layer to our output space\n","        self.output = nn.Linear(hidden_size, output_size)\n","\n","    def forward(self, input_sequence):\n","        emb = self.embedding(input_sequence)\n","        emb_drop = self.emb_dropout(emb)\n","        rnn_out, _  = self.rnn(emb_drop)\n","        rnn_out_drop = self.out_dropout(rnn_out)\n","        output = self.output(rnn_out_drop).permute(0,2,1)\n","        return output"]},{"cell_type":"markdown","metadata":{"id":"pdTTIVp4KUft"},"source":["### Params"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZoTPBeFkKUft"},"outputs":[],"source":["hid_size = 300\n","emb_size = 300\n","\n","lr = 2\n","clip = 5\n","patience = 3\n","n_epochs = 100\n","emb_dropout = 0.25\n","out_dropout = 0.25\n","\n","vocab_len = len(lang.word2id)\n","\n","model = LM_LSTM_drop(emb_size, hid_size, vocab_len, pad_index=lang.word2id[\"<pad>\"], emb_dropout=emb_dropout, out_dropout=out_dropout).to(device)\n","model.apply(init_weights)\n","\n","optimizer = optim.SGD(model.parameters(), lr=lr)\n","criterion_train = nn.CrossEntropyLoss(ignore_index=lang.word2id[\"<pad>\"])\n","criterion_eval = nn.CrossEntropyLoss(ignore_index=lang.word2id[\"<pad>\"], reduction='sum')\n"]},{"cell_type":"markdown","metadata":{"id":"SwSvO-AxKUft"},"source":["### Train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EMkrZL9aKUft"},"outputs":[],"source":["if (run_exp[2]):\n","  train(model=model, optimizer=optimizer, exp_name=name, clip=clip, epochs=n_epochs, patience=patience, batch_size=batch_size)\n","\n","  #save all parameters (lr, emb_size, hid_size, optimizer, clip, patience, n_epochs)\n","  with open(tensorboard_folder+'/'+name+'/params.txt', 'w') as f:\n","      f.write('lr: {}\\n'.format(optimizer.param_groups[0]['lr']))\n","      f.write('weight_decay: {}\\n'.format(optimizer.param_groups[0]['weight_decay']))\n","      f.write('optimizer: {}\\n'.format(optimizer))\n","      f.write('emb_dropout: {}\\n'.format(emb_dropout))\n","      f.write('out_dropout: {}\\n'.format(out_dropout))\n","      f.write('emb_size: {}\\n'.format(emb_size))\n","      f.write('hid_size: {}\\n'.format(hid_size))\n","      f.write('optimizer: {}\\n'.format(optimizer))\n","      f.write('clip: {}\\n'.format(clip))\n","      f.write('patience: {}\\n'.format(patience))\n","      f.write('n_epochs: {}\\n'.format(n_epochs))\n","      f.write('batch_size: {}\\n'.format(batch_size))"]},{"cell_type":"markdown","metadata":{"id":"f9wKKMLgKUfu"},"source":["## LSTM AdamW"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wE6ODjVqKUfu"},"outputs":[],"source":["name = \"LSTM_AdamW\""]},{"cell_type":"markdown","metadata":{"id":"ODrTqA4qKUfu"},"source":["### Params"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3KpnICPnKUfu"},"outputs":[],"source":["hid_size = 300\n","emb_size = 300\n","\n","clip = 5\n","patience = 3\n","n_epochs = 100\n","emb_dropout = 0.25\n","out_dropout = 0.25\n","\n","vocab_len = len(lang.word2id)\n","\n","model = LM_LSTM_drop(emb_size, hid_size, vocab_len, pad_index=lang.word2id[\"<pad>\"], emb_dropout=emb_dropout, out_dropout=out_dropout).to(device)\n","model.apply(init_weights)\n","\n","optimizer = optim.AdamW(model.parameters())\n","criterion_train = nn.CrossEntropyLoss(ignore_index=lang.word2id[\"<pad>\"])\n","criterion_eval = nn.CrossEntropyLoss(ignore_index=lang.word2id[\"<pad>\"], reduction='sum')"]},{"cell_type":"markdown","metadata":{"id":"cc3LKXHiKUfv"},"source":["### Train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hQYvd6-gKUfv"},"outputs":[],"source":["if (run_exp[3]):\n","  train(model=model, optimizer=optimizer, exp_name=name, clip=clip, epochs=n_epochs, patience=patience, batch_size=batch_size)\n","\n","  #save all parameters (lr, emb_size, hid_size, optimizer, clip, patience, n_epochs)\n","  with open(tensorboard_folder+'/'+name+'/params.txt', 'w') as f:\n","      f.write('lr: {}\\n'.format(optimizer.param_groups[0]['lr']))\n","      f.write('weight_decay: {}\\n'.format(optimizer.param_groups[0]['weight_decay']))\n","      f.write('optimizer: {}\\n'.format(optimizer))\n","      f.write('emb_dropout: {}\\n'.format(emb_dropout))\n","      f.write('out_dropout: {}\\n'.format(out_dropout))\n","      f.write('emb_size: {}\\n'.format(emb_size))\n","      f.write('hid_size: {}\\n'.format(hid_size))\n","      f.write('optimizer: {}\\n'.format(optimizer))\n","      f.write('clip: {}\\n'.format(clip))\n","      f.write('patience: {}\\n'.format(patience))\n","      f.write('n_epochs: {}\\n'.format(n_epochs))\n","      f.write('batch_size: {}\\n'.format(batch_size))"]},{"cell_type":"markdown","metadata":{"id":"azY7ky3zKUfv"},"source":["# Part 2"]},{"cell_type":"markdown","metadata":{"id":"LELp6cNYKUfw"},"source":["**Mandatory requirements**: For the following experiments the perplexity must be below 250 (***PPL < 250***) and it should be lower than the one achieved in Part 1.1 (i.e. base LSTM).\n","\n","Starting from the `LM_RNN` in which you replaced the RNN with a LSTM model, apply the following regularisation techniques:\n","- Weight Tying\n","- Variational Dropout (no DropConnect)\n","- Non-monotonically Triggered AvSGD\n","\n","These techniques are described in [this paper](https://openreview.net/pdf?id=SyyGPP0TZ)."]},{"cell_type":"markdown","metadata":{"id":"9IlN6rikKUfw"},"source":["## LSTM Weight tying"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"glJubxuXKUfw"},"outputs":[],"source":["name = \"LSTM_WT_higherLR\""]},{"cell_type":"markdown","metadata":{"id":"ZOK2jVWtKUfw"},"source":["### Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f1Wf1WcaKUfw"},"outputs":[],"source":["class LM_LSTM_WT(nn.Module):\n","    def __init__(self, emb_size, hidden_size, output_size, pad_index=0, out_dropout=0.1,\n","                 emb_dropout=0.1, n_layers=1):\n","        super(LM_LSTM_WT, self).__init__()\n","        # Token ids to vectors, we will better see this in the next lab\n","        self.embedding = nn.Embedding(output_size, emb_size, padding_idx=pad_index)\n","        # Pytorch's RNN layer: https://pytorch.org/docs/stable/generated/torch.nn.RNN.html\n","        self.rnn = nn.LSTM(emb_size, hidden_size, n_layers, bidirectional=False, batch_first=True)\n","        self.pad_token = pad_index\n","        # Linear layer to project the hidden layer to our output space\n","        self.output = nn.Linear(hidden_size, output_size)\n","        # Weight tying\n","        self.output.weight = self.embedding.weight\n","\n","    def forward(self, input_sequence):\n","        emb = self.embedding(input_sequence)\n","        rnn_out, _  = self.rnn(emb)\n","        output = self.output(rnn_out).permute(0,2,1)\n","        return output"]},{"cell_type":"markdown","metadata":{"id":"EDb8ZwFTKUfx"},"source":["### Params"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QOmmK0RXKUfx"},"outputs":[],"source":["hid_size = 300\n","emb_size = 300\n","\n","lr = 2\n","clip = 5\n","patience = 3\n","n_epochs = 100\n","\n","vocab_len = len(lang.word2id)\n","\n","model = LM_LSTM_WT(emb_size, hid_size, vocab_len, pad_index=lang.word2id[\"<pad>\"]).to(device)\n","model.apply(init_weights)\n","\n","optimizer = optim.SGD(model.parameters(), lr=lr)\n","criterion_train = nn.CrossEntropyLoss(ignore_index=lang.word2id[\"<pad>\"])\n","criterion_eval = nn.CrossEntropyLoss(ignore_index=lang.word2id[\"<pad>\"], reduction='sum')"]},{"cell_type":"markdown","metadata":{"id":"ECf5rwDVKUfx"},"source":["### Train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6-c1YbGNKUfy"},"outputs":[],"source":["if (run_exp[4]):\n","  train(model=model, optimizer=optimizer, exp_name=name, clip=clip, epochs=n_epochs, patience=patience, batch_size=batch_size)\n","\n","  #save all parameters (lr, emb_size, hid_size, optimizer, clip, patience, n_epochs)\n","  with open(tensorboard_folder+'/'+name+'/params.txt', 'w') as f:\n","      f.write('lr: {}\\n'.format(optimizer.param_groups[0]['lr']))\n","      f.write('weight_decay: {}\\n'.format(optimizer.param_groups[0]['weight_decay']))\n","      f.write('optimizer: {}\\n'.format(optimizer))\n","      f.write('emb_size: {}\\n'.format(emb_size))\n","      f.write('hid_size: {}\\n'.format(hid_size))\n","      f.write('optimizer: {}\\n'.format(optimizer))\n","      f.write('clip: {}\\n'.format(clip))\n","      f.write('patience: {}\\n'.format(patience))\n","      f.write('n_epochs: {}\\n'.format(n_epochs))\n","      f.write('batch_size: {}\\n'.format(batch_size))"]},{"cell_type":"markdown","metadata":{"id":"6xE0S40RKUfy"},"source":["## LSTM WT Variational Dropout"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kneLDKmuKUfy"},"outputs":[],"source":["name = \"LSTM_WT_VD_higherLR\""]},{"cell_type":"markdown","metadata":{"id":"l9Vsqn0nKUfy"},"source":["### Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u9ZeRO19KUfy"},"outputs":[],"source":["class VariationalDropout(nn.Module):\n","    def __init__(self, dropout_probability: float,):\n","        super().__init__()\n","        self.p = dropout_probability\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        if not self.training or self.p <= 0.:\n","            return x\n","\n","        batch_size = x.size(0)\n","\n","        mask = x.new_empty(batch_size, 1, x.size(2), requires_grad=False).bernoulli_(1 - self.p)\n","\n","        mask = mask.expand_as(x)\n","        x = x.mul(mask).div(1.0 - self.p)\n","\n","        return x\n","\n","class LM_LSTM_WT_VD(nn.Module):\n","    def __init__(self, emb_size, hidden_size, output_size, pad_index=0, out_dropout=0.1,\n","                 emb_dropout=0.1, n_layers=1):\n","        super(LM_LSTM_WT_VD, self).__init__()\n","        # Token ids to vectors, we will better see this in the next lab\n","        self.embedding = nn.Embedding(output_size, emb_size, padding_idx=pad_index)\n","        # Pytorch's RNN layer: https://pytorch.org/docs/stable/generated/torch.nn.RNN.html\n","        self.rnn = nn.LSTM(emb_size, hidden_size, n_layers, bidirectional=False, batch_first=True)\n","        self.pad_token = pad_index\n","        # Linear layer to project the hidden layer to our output space\n","        self.output = nn.Linear(hidden_size, output_size)\n","        # Weight tying\n","        self.output.weight = self.embedding.weight\n","        # variational dropout\n","        self.emb_dropout = VariationalDropout(emb_dropout)\n","        # variational dropout\n","        self.out_dropout = VariationalDropout(out_dropout)\n","\n","\n","    def forward(self, input_sequence):\n","        emb = self.embedding(input_sequence)\n","        emb_drop = self.emb_dropout(emb)\n","        rnn_out, _  = self.rnn(emb_drop)\n","        out_drop = self.out_dropout(rnn_out)\n","        output = self.output(out_drop).permute(0,2,1)\n","        return output"]},{"cell_type":"markdown","metadata":{"id":"Iw5nA1AoKUfz"},"source":["### Params"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VDsvT1GHKUfz"},"outputs":[],"source":["hid_size = 300\n","emb_size = 300\n","\n","lr = 2\n","clip = 5\n","patience = 3\n","n_epochs = 100\n","emb_dropout = 0.2\n","out_dropout = 0.2\n","\n","vocab_len = len(lang.word2id)\n","\n","model = LM_LSTM_WT_VD(emb_size, hid_size, vocab_len, pad_index=lang.word2id[\"<pad>\"], emb_dropout=emb_dropout, out_dropout=out_dropout).to(device)\n","model.apply(init_weights)\n","\n","optimizer = optim.SGD(model.parameters(), lr=lr)\n","criterion_train = nn.CrossEntropyLoss(ignore_index=lang.word2id[\"<pad>\"])\n","criterion_eval = nn.CrossEntropyLoss(ignore_index=lang.word2id[\"<pad>\"], reduction='sum')"]},{"cell_type":"markdown","metadata":{"id":"y6lfB_rjKUfz"},"source":["### Train"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K6dwFE-bKUfz","outputId":"ec4c478f-3d09-49a7-cedb-4b877ce3bd20"},"outputs":[{"name":"stderr","output_type":"stream","text":["PPL: 146.082096:  73%|███████▎  | 72/99 [33:30<12:34, 27.96s/it]"]}],"source":["if (run_exp[5]):\n","  train(model=model, optimizer=optimizer, exp_name=name, clip=clip, epochs=n_epochs, patience=patience, batch_size=batch_size)\n","\n","  #save all parameters (lr, emb_size, hid_size, optimizer, clip, patience, n_epochs)\n","  with open(tensorboard_folder+'/'+name+'/params.txt', 'w') as f:\n","      f.write('lr: {}\\n'.format(optimizer.param_groups[0]['lr']))\n","      f.write('weight_decay: {}\\n'.format(optimizer.param_groups[0]['weight_decay']))\n","      f.write('optimizer: {}\\n'.format(optimizer))\n","      f.write('emb_dropout: {}\\n'.format(emb_dropout))\n","      f.write('out_dropout: {}\\n'.format(out_dropout))\n","      f.write('emb_size: {}\\n'.format(emb_size))\n","      f.write('hid_size: {}\\n'.format(hid_size))\n","      f.write('optimizer: {}\\n'.format(optimizer))\n","      f.write('clip: {}\\n'.format(clip))\n","      f.write('patience: {}\\n'.format(patience))\n","      f.write('n_epochs: {}\\n'.format(n_epochs))\n","      f.write('batch_size: {}\\n'.format(batch_size))"]},{"cell_type":"markdown","metadata":{"id":"EYIyraIJKUf0"},"source":["## LSTM Non-monotonically Triggered AvSGD"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WBSO3rmuKUf0"},"outputs":[],"source":["name = \"LSTM_NT_AvSGD_higherLR\""]},{"cell_type":"markdown","metadata":{"id":"a6d0QxPnKUf0"},"source":["### Optimizer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Cx0GGlAqKUf0"},"outputs":[],"source":["# put into functions\n","class NT_AvSGD(optim.ASGD):\n","    def __init__(self, model, lr=1, L=100, n=5):\n","        super(NT_AvSGD, self).__init__(model.parameters(), lr=lr, t0=float('inf'))\n","        self.temp = {}\n","        self.logs = []\n","        self.dev_loader = DataLoader(dev_dataset, batch_size=batch_size, collate_fn=partial(collate_fn, pad_token=lang.word2id[\"<pad>\"]))\n","        self.T = 0\n","        self.t = 0\n","        self.k = 0\n","        self.L = L\n","        self.n = n\n","        self.mu = 1\n","        self.model = model\n","\n","    def step(self, closure=None):\n","        super(NT_AvSGD, self).step(closure)\n","        with torch.no_grad():\n","          #calculate validation PPL\n","          if self.k % self.L == 0 and self.T==0:\n","              ppl_dev, _ = eval_loop(self.dev_loader, criterion_eval, self.model)\n","              self.model.train()\n","              if self.t>self.n and ppl_dev > min(self.logs[:self.t-self.n]):\n","                  self.T = self.k\n","                  # set t0 of ASGD\n","                  self.param_groups[0]['t0'] = 0\n","                  print(\"averaging at k \"+self.k)\n","              self.logs.append(ppl_dev)\n","              self.t += 1\n","          self.k += 1\n","\n","    def average(self):\n","        if self.T == 0:\n","            print(\"No need to average\")\n","            return\n","        with torch.no_grad():\n","            # use ax computed in ASGD\n","            for prm in self.model.parameters():\n","                self.temp[prm] = prm.data.clone()\n","                prm.data = self.state[prm]['ax'].clone()\n","\n","    def restore(self):\n","        if self.T == 0:\n","            print(\"No need to restore\")\n","            return\n","        with torch.no_grad():\n","            for prm in self.model.parameters():\n","                prm.data = self.temp[prm].clone()\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"1n8reBrqKUf1"},"source":["### Params"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tSIqoz7nKUf1"},"outputs":[],"source":["hid_size = 300\n","emb_size = 300\n","\n","lr = 1\n","clip = 5\n","patience = 10\n","n_epochs = 100\n","L = 165\n","n=5\n","emb_dropout = 0.2\n","out_dropout = 0.2\n","\n","vocab_len = len(lang.word2id)\n","\n","model = LM_LSTM_WT_VD(emb_size, hid_size, vocab_len, pad_index=lang.word2id[\"<pad>\"], emb_dropout=emb_dropout, out_dropout=out_dropout).to(device)\n","model.apply(init_weights)\n","\n","optimizer = NT_AvSGD(lr=lr, L=L, n=n, model = model)\n","criterion_train = nn.CrossEntropyLoss(ignore_index=lang.word2id[\"<pad>\"])\n","criterion_eval = nn.CrossEntropyLoss(ignore_index=lang.word2id[\"<pad>\"], reduction='sum')"]},{"cell_type":"markdown","metadata":{"id":"a5A2NO-YKUf2"},"source":["### Train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zJZfYTW8KUf2"},"outputs":[],"source":["if (run_exp[6]):\n","  train(model=model, optimizer=optimizer, exp_name=name, clip=clip, epochs=n_epochs, patience=patience, batch_size=batch_size)\n","  optimizer.average()\n","\n","  #calculate test PPL\n","  test_loader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=partial(collate_fn, pad_token=lang.word2id[\"<pad>\"]))\n","  ppl_test, _ = eval_loop(test_loader, criterion_eval, model)\n","  print('Test ppl: ', ppl_test)\n","  writer = SummaryWriter(tensorboard_folder+'/'+name)\n","  writer.add_scalar('PPL/test', ppl_test, 1)\n","  writer.close()\n","\n","  #save model\n","  torch.save(model.state_dict(), models_folder+'/'+name+'.pt')\n","\n","  #save all parameters (lr, emb_size, hid_size, optimizer, clip, patience, n_epochs)\n","  with open(tensorboard_folder+'/'+name+'/params.txt', 'w') as f:\n","      f.write('lr: {}\\n'.format(lr))\n","      f.write('emb_size: {}\\n'.format(emb_size))\n","      f.write('hid_size: {}\\n'.format(hid_size))\n","      f.write('emb_dropout: {}\\n'.format(emb_dropout))\n","      f.write('out_dropout: {}\\n'.format(out_dropout))\n","      f.write('optimizer: {}\\n'.format(optimizer))\n","      f.write('clip: {}\\n'.format(clip))\n","      f.write('patience: {}\\n'.format(patience))\n","      f.write('L: {}\\n'.format(L))\n","      f.write('n: {}\\n'.format(n))\n","      f.write('n_epochs: {}\\n'.format(n_epochs))\n","      f.write('batch_size: {}\\n'.format(batch_size))"]},{"cell_type":"markdown","metadata":{"id":"PpG2KEMoKUf2"},"source":["# Results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9PuGOpeKKUf2"},"outputs":[],"source":["# Load the TensorBoard notebook extension\n","%load_ext tensorboard\n","%tensorboard --logdir 'drive/MyDrive/MASTER_AIS/labs/NLU-2024-Labs/assignment_1/tensorboard'"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["864ZIfxlKUfo","3nQAvSUGKUfq","njUBeO_LKUfq","nH-j_QD8KUfs","pdTTIVp4KUft","f9wKKMLgKUfu","9IlN6rikKUfw"],"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}